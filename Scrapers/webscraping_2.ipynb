{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sEjTHj5pe4P",
        "outputId": "6aa1fc49-56e9-47e5-ace5-93d1669ba9b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/documents’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir $'/content/documents'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scraping Recipes - All Recipes\n",
        "import html\n",
        "from urllib.request import Request,urlopen\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from urllib.parse import urljoin\n",
        "#def quick_save(site_str, recipes):\n",
        "    #save_recipes(\n",
        "        #path.join(config.path_data, 'recipes_raw_{}.json'.format(site_str)),\n",
        "        #recipes)\n",
        "\n",
        "base_url = 'https://www.epicurious.com/search?content=recipe&page='\n",
        "prefix_url = 'https://www.epicurious.com'\n",
        "\n",
        "recipe_titles = []\n",
        "recipe_img_links = []\n",
        "recipe_ingredients = []\n",
        "recipe_intructions = []\n",
        "\n",
        "with open(\"dataset_epi.json\", mode='w', encoding='utf-8') as f:\n",
        "    json.dump([], f)\n",
        "\n",
        "recipe_count = 1\n",
        "\n",
        "for i in range(1,3):# 2032):\n",
        "\n",
        "    print(\"Page \"+str(i)+\" scraping started!!!\")\n",
        "\n",
        "    url = base_url + str(i)\n",
        "    req = Request(url, headers={'User-Agent':'Mozilla/5.0'})\n",
        "    webpage = urlopen(req).read()\n",
        "    page_soup = soup(webpage,\"html.parser\")\n",
        "\n",
        "    recipe_links_on_page = []\n",
        "\n",
        "    recipe_links = page_soup.find_all('h4', {'class': 'hed'})\n",
        "    #print(recipe_links)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PhO3HLMURcU",
        "outputId": "d3ea7fa4-36ff-4b66-9229-70cfec9afbd5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 1 scraping started!!!\n",
            "Page 2 scraping started!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    for link in recipe_links:\n",
        "      #recipe_links_on_page.append(prefix_url + link.find('a')['href'])\n",
        "      the_link = link.find('a')['href']\n",
        "      #print(thelink)\n",
        "      new_link = urljoin(prefix_url,the_link)\n",
        "      #print(new_link)\n",
        "      recipe_links_on_page.append(new_link)\n",
        "\n",
        "    #print(recipe_links_on_page)"
      ],
      "metadata": {
        "id": "eDHt2MWdaGlS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    for recipe_url in recipe_links_on_page:\n",
        "        req_recipe = Request(recipe_url, headers={'User-Agent':'Mozilla/5.0'})\n",
        "        webpage_recipe = urlopen(req_recipe).read()\n",
        "        page_soup_recipe = soup(webpage_recipe,\"html.parser\")\n",
        "\n",
        "        # Name of Recipe\n",
        "        name = page_soup_recipe.find('h1').text\n",
        "        recipe_titles.append(name)\n",
        "\n",
        "    #print(recipe_titles)\n",
        "        # Ingredients\n",
        "        ingredients = page_soup_recipe.find(\"div\",{ \"class\" : \"BaseWrap-sc-gjQpdd BaseText-ewhhUZ Description-cSrMCf iUEiRd ioVvSX fsKnGI\" })\n",
        "         #.find('h3', class_='ingredient-groups').find_all('h2', {'class':'ingredient'})\n",
        "        #print(ingredients)\n",
        "        temp_ingredients = []\n",
        "\n",
        "        for ingredient in ingredients:\n",
        "            temp_ingredients.append(ingredient.text.strip())\n",
        "\n",
        "        recipe_ingredients.append(temp_ingredients)\n",
        "    #print(recipe_ingredients)"
      ],
      "metadata": {
        "id": "FrzyS--6daVy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "        # Recipe\n",
        "        #recipes = page_soup_recipe.find('p')\n",
        "        recipes = page_soup_recipe.find('ol', {\"class\" : \"InstructionGroupWrapper-bqiIwp ccobUj\"})\n",
        "        #recipes_step=page_soup_recipe.find('li', {\"class\" : \"InstructionListWrapper-dcpygI kinFAs \"})\n",
        "        #print(recipes_step)\n",
        "        temp_recipe = []\n",
        "\n",
        "        for recipe in recipes:\n",
        "            temp_recipe.append(recipe.text.strip())\n",
        "\n",
        "        recipe_intructions.append(temp_recipe)\n",
        "\n",
        "        #print(recipe_intructions)\n"
      ],
      "metadata": {
        "id": "DiOackBftoJK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Image\n",
        "        #temp_img = page_soup_recipe.find('div',class_='social-img').find('img')['srcset']\n",
        "        #recipe_img_links.append(temp_img)\n",
        "\n",
        "        #IMGS_FOLDER = \"images\\epi\"\n",
        "\n",
        "        #img_name = IMGS_FOLDER + str(recipe_count) + '.jpg'\n",
        "        #urllib.request.urlretrieve(temp_img, img_name)\n",
        "# \"Recipe_img_link\" : temp_img\n",
        "\n",
        "    with open(\"dataset_epi.json\", mode = 'w', encoding = 'utf-8') as feedsjson:\n",
        "              entry = {\n",
        "                     'Recipe_number': recipe_count,\n",
        "                     'Recipe_name': name,\n",
        "                     'Recipe_ingredients': temp_ingredients,\n",
        "                     \"Recipe_instructions\": temp_recipe\n",
        "              }\n",
        "              feeds=[]\n",
        "              feeds.append(entry)\n",
        "              json.dump(feeds,feedsjson)\n",
        "\n",
        "    print(\"Recipe \"+str(recipe_count)+\" Scrapped\")\n",
        "    recipe_count += 1\n",
        "\n",
        "    print(\"Page \"+str(i)+\" Scraped!!!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "aV26T6bKZf-n",
        "outputId": "a5a66bc2-1519-4f3e-a17f-e2d2c10d5406"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-fbb3316b36f0>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    with open(\"dataset_epi.json\", mode = 'w', encoding = 'utf-8') as feedsjson:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    }
  ]
}
